<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="DEMON challenge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DEMON: Demonstrative Instruction Following Challenge</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <!--  <script src="./static/js/index.js"></script>-->
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">DEMON: Demonstrative Instruction Following Challenge</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <span>Zhiqi Ge</span>,</span>
                            <span class="author-block">
                                <span>Juncheng Li</span>,</span>
                            <span class="author-block">
                                <span>Qifan Yu</span>,</span>
                            <span class="author-block">
                                <span>Wei Zhou</span>,</span>
                            <span class="author-block">
                                <span>Siliang Tang</span>,</span>
                            <span class="author-block">
                                <span>Yueting Zhuang
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Zhejiang University</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- Data Link. -->
                                <span class="link-block">
                                    <a href="https://drive.google.com/drive/folders/1x3O7brcJDEMONXJS2c6LLUpG5Xx0mg1WkRG?usp=drive_link "
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-images"></i>
                                        </span>
                                        <span>Data</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/DCDmllm/DEMON-challenge/tree/main"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-code"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Participate Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/DCDmllm/DEMON-challenge/tree/main"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-user"></i>
                                        </span>
                                        <span>Participate</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop" style="max-width: 70%!important;">
            <div class="hero-body">

                <img src="static/images/demo.svg" width="100%" alt="">
                <h2 class="subtitle has-text-centered">
                    Figure 1: Demonstrations and task taxonomy of the proposed DEMON challenge.
                </h2>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>To facilitate research in interleaved vision-language instruction following, we build
                            <strong>DEMON</strong>, a comprehensive challenge of 29 tasks with varied, demonstrative 
                            instructions in a uniÔ¨Åed instruction-response format, covering 20 diverse scenarios.
                            As depicted in Figure 1, DEMON has three important properties:</p>
                        <ul>
                            <li><strong>Interleaved vision-language context,</strong> all the instructions comprise
                                sequences of interconnected images and texts, such as storyboards with scripts, and
                                textbooks with diagrams.</li>
                            <li><strong>Diverse forms of complex instructions,</strong> ranging from generating dialogue
                                for comics, identifying disparities in surveillance images, to engaging in
                                conversational embodied task.</li>
                            <li><strong>Vast range of instruction-following scenarios,</strong> the benchmark
                                encompasses multiple real-world scenarios, including cartoons, industrial images,
                                driving recordings, recipes, <em>etc</em>.</li>
                        </ul>

                    </div>
                </div>
            </div>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Challenge Task</h2>
                    <div class="content has-text-justified">

                        <p><strong>Visual Storytelling.</strong> This task involves creating a coherent narrative based
                            on a series of images presented in sequence. It tests the model's ability to understand
                            context, sequence, and the progression of events from visual inputs, requiring the
                            construction or prediction of a story continuation.</p>

                        <p><strong>Multi-Modal Dialogue.</strong> In this task, the model must engage in a dialogue that
                            necessitates the interpretation of both visual content and text. It assesses the model's
                            ability to integrate multimodal information and apply it within a conversational context to
                            make decisions or respond to queries.</p>

                        <p><strong>Visual Relation Inference.</strong> The objective is to identify and articulate the
                            relationships or changes between two pictures. This task challenges the model to observe
                            details and analyze the visual information to determine counts, positions, or interactions.
                        </p>

                        <p><strong>Text-Rich Images QA.</strong> This task requires the model to extract and comprehend
                            information from text-heavy images, such as slides or documents. It demands capabilities in
                            text recognition within images and the subsequent application of this information to answer
                            related questions.</p>

                        <p><strong>Multi-Image Reasoning.</strong> The model is tasked with analyzing multiple images to
                            make judgments about them, such as assessing whether they share a similar style or theme. It
                            tests the model's ability to process and compare visual elements, and to make inferences
                            based on visual data.</p>

                        <p><strong>Multi-Modal Cloze.</strong> This task presents the model with a sequence of images or
                            text with a missing element, requiring the correct identification of the subsequent part
                            from the provided options. It evaluates the model's understanding of narrative structure and
                            context comprehension to logically fill in the missing pieces.</p>

                        <p><strong>Knowledge Grounded QA.</strong> The model is challenged to answer questions based on
                            complex diagrams or authoritative text sources. It necessitates a comprehensive
                            understanding of the material and the extraction of relevant information to provide accurate
                            responses.
                        </p>
                    </div>
                </div>
            </div>
            <!-- / Paper video.
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Data</h2>
                    <div class="content has-text-justified">
                        <p>
                            You can download the sample data now and the full data is coming soon.
                        </p>
                    </div>
                </div>
            </div> -->
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            Borrowed from <a href="https://github.com/nerfies/nerfies.github.io">source code</a>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>